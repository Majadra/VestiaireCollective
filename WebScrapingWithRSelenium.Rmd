---
title: "Scraping from vestiairecollective"
author: "Ran K"
date: "8/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(RSelenium)
library(rvest)
library(stringr)
# 1. Lunch docker and set up a chrome enironment, the command is "docker run --name chrome -v /dev/shm:/dev/shm -d -p 4445:4444 -p 5901:5900 selenium/standalone-chrome-debug:latest"
# 2. "docker ps -a" to verify that it is up
# 3. Lunch tightVNC and connect to out server.
```

```{r Server Preparation, echo = FALSE}
remDr <- remoteDriver(remoteServerAddr = "192.168.99.100", port = 4445L, browser = "chrome")
remDr$open()
```

```{r Navigating to login screen, echo=FALSE}
remDr$navigate("https://us.vestiairecollective.com/")
main_webpage <- remDr$findElement("css", "body")
Login_button <- remDr$findElement(using = "css", ".d-lg-none+ .d-lg-block .resetButton")
main_webpage$mouseMoveToLocation(webElement = Login_button) 
main_webpage$click(1)
# Now i need to manuly connect to a profile
```

``` {r Functions we will use}
GoToSleep <- function() {
  print("Going to sleep")
  Sys.sleep(runif(1,8,9))
}

SortByPriceDesc <- function() {
  sort_button <- remDr$findElement(using = "css",value = ".catalogSort__button")
  remDr$mouseMoveToLocation(webElement = sort_button)
  remDr$click(1)
  sort_button <- remDr$findElement(using = "id", value = "mat-radio-4")
  remDr$mouseMoveToLocation(webElement = sort_button)
  remDr$click(1)
}

SortByPriceAsc <- function() {
  sort_button <- remDr$findElement(using = "css",value = ".catalogSort__button")
  remDr$mouseMoveToLocation(webElement = sort_button)
  remDr$click(1)
  sort_button <- remDr$findElement(using = "id", value = "mat-radio-3")
  remDr$mouseMoveToLocation(webElement = sort_button)
  remDr$click(1)
}

SetMinPrice <- function(PriceMin) {
  Min_button <- remDr$findElement(using = "css",value = "#price-range-min")
  remDr$mouseMoveToLocation(webElement = Min_button)
  remDr$click(2)
  Min_button$sendKeysToElement(list(PriceMin))
} #not needed

SetMaxPrice <- function(PriceMax) {
  Max_button <- remDr$findElement(using = "css",value = "#price-range-max") 
  remDr$mouseMoveToLocation(webElement = Max_button)
  remDr$click(2)
  Max_button$sendKeysToElement(list(PriceMax))
} #not needed

```

``` {r Initialing variables for scraping}
# Initialising  the variables we will collect.
Price <- list()
Likes <- list()
Other_Details <- list() # all other information (will be stored in a vector)
Link_list <- read.csv("LinkList.csv")
Link_list <- Link_list[,2]
Designer_List <- read.csv ("DesignerList.csv")


# This will be the database which will contain all the individual products
df <- 
  data.frame(Date_sold = as.character(),
             Likes = as.numeric(),
             Designer = as.character(),
             Model = as.character(),
             Price_sold = as.character(), 
             # As the site uses different currencies, ill do the exchange rate after.
             Condition = as.character(),
             Material = as.character(),
             Color = as.character(),
             Width = as.character(), # If noted
             Height = as.character(), # If noted
             Depth = as.character(), # If noted
             Size = as.character(), 
             # Will need to converge all sizes into EU sizes after.
             Location = as.character(),
             Seller = as.character(), # Is a part of the same string as location
             Sold_with = as.character(), # A vector of items
             Online_since = as.character(),
             Reference = as.numeric()
             )
```

``` {r Preparing list of designers}
### Prepering the List of designers
#This is not needed as i have allready saved it to csv
remDr$navigate("https://us.vestiairecollective.com/women-bags/handbags/#sold=1")
Labels <- remDr$findElements(using = "css","label")
Labels <- sapply(Labels,function(x){x$getElementText()[[1]]})
Designer_List <- Labels[7:106] # This was checked manually
Designer_List <- as.data.frame(Designer_List)
Designer_List["Number"] <- as.numeric()
for (i in 1:nrow(Designer_List)) {
  Designer_List$Number[i] <- as.numeric(regmatches(Designer_List[i,1],
                               regexpr("(?<=\\()\\d+(?=\\))",Designer_List[i,1],perl = TRUE)))
  Designer_List$Designers[i] <- gsub("& ","",Designer_List[i,3])
  Designer_List$Designers[i] <- gsub(" ","-",sub(" \\(\\d+\\)","",Designer_List[i,3]))
  Designer_List$Designers[i] <- gsub("[éè]","e",Designer_List[i,3])
   
}  

# Its possible to save it as Designer_List.csv

``` 

``` {r link scraping function}
MoveThroughPages_ScrapeAndAppendToList <- function(Link_list)
{
  
for (Index_page in 1:17) 
  { # There are a maximum of 17 pages of 60 articles per page.
  print(paste0("Scraping links from page ",Index_page))
  Current_page <- remDr$getCurrentUrl()
  remDr$navigate(sub("/p-\\d{1,2}/#|/#",paste0("/p-",Index_page,"/#"),Current_page))
  
  print(Current_page)
  GoToSleep() 
  
  print("Scraping links")
  # Collecting the lonks for this individual page
  Links_in_page <- # Collecting all links
    remDr$findElements(using = "css", "meta") 
  Links_in_page <- # Grouping in a list
    unlist(sapply(Links_in_page,function(x){x$getElementAttribute('content')}))
  Links_in_page <- # Filtering to find only links to individual heals
    Links_in_page[which(grepl("^us.vestiairecollective.com/women-bags/handbags/.+-[[:digit:]]{3,}.shtml$",Links_in_page) == TRUE)]
  
  print("Appending")
  Appended_item_index <- 0
  for (Link in Links_in_page) 
    {
      
    if (is.na(match(Link,Link_list)) == TRUE) 
      # Here we will check for duplicates
      # We will add each link to our final list only if it does not 
      # already appears there
    {
      Link_list <- append(Link_list,Link)
      Appended_item_index <- Appended_item_index + 1
    }
  }
  print(paste0(Appended_item_index," items added"))
  assign("Link_list",Link_list,envir = .GlobalEnv) 
  # This is to have "Link_list" change in the global environement
  if (length(Links_in_page) < 50) {
    print("This is the last page in the selection")
    break() # As we have 60 links in a full page,
    # a page with less links means that that we cant move any further
  }
}
}
```


``` {r Scraping links for future content scraping}
for (Index_designer in 1:nrow(Designer_List)) 
  {
  # Here ill create several if statements which will change how we approch to task of scraping
  # links of indiviual products
  
  # As we are confined into a presentation of a maximum of 1000 articals - (16 pages of 60
  # articles and a 17th page with 40 articles), we can fork all products from small designers with
  # less then 1k products sold in 1 go. for designers with 1-2k products we will not refilter but
  # instead scrape once with sorting by ascending price and once with sorting by descending price.
  # for bigger designers ill use a filter of price, with bigger increments for the big designers 
  # and small increments for the huge ones like chanel with 14k products sold.
  if (Designer_List$Number[Index_designer] <= 1000) { 
    # small designers, we can scrape in one go.
    
    print(paste0(Designer_List$Designers[Index_designer],
                 " - 1000 and less products sold"))
    
    
    next() ########## As i Already finished this category ill skip 
   
    
    
    remDr$navigate(paste0("https://us.vestiairecollective.com/women-bags/handbags/",
                          Designer_List$Designers[Index_designer],"/#sold=1"))

    GoToSleep()
    MoveThroughPages_ScrapeAndAppendToList(Link_list)
  }
  if (Designer_List$Number[Index_designer]>1000) {
    print(paste0(Designer_List$Designers[Index_designer],
                 " - over 1000 articles sold - filtering by price needed needed"))
    remDr$navigate(paste0("https://us.vestiairecollective.com/women-bags/handbags/",
                          Designer_List$Designers[Index_designer],"/#sold=1"))
    GoToSleep()
    
    stopMarker <- remDr$findElement(using = "css",
                                    value = ".vc-price-range__col--max .vc-price-range__label")
    stopMarker <- stopMarker$getElementText()
    stopMarker <- as.numeric(gsub("[€ ]|,\\d{1,2}","",stopMarker,perl = TRUE)) + 1
    print(paste0("Stop marker is - ",stopMarker))
    # This is the product who was sold for the highest price
    
    minPrice <- 0.01
    maxPrice <- 10.01
    while (stopMarker >= maxPrice & maxPrice <= 500.01)
    {
      remDr$navigate(paste0("https://us.vestiairecollective.com/women-bags/handbags/",
                          Designer_List$Designers[Index_designer],                     "/#sold=1_priceMin=",as.character(minPrice*100),"_priceMax=",as.character(maxPrice*100)))
      GoToSleep()
      print(paste0("starting to scrape between ",minPrice," and ",maxPrice))
      MoveThroughPages_ScrapeAndAppendToList(Link_list)

      minPrice <- minPrice + 10
      maxPrice <- maxPrice + 10
    }
    maxPrice <- maxPrice + 40
        while (stopMarker >= maxPrice & maxPrice <= 1000.01)
    {
      remDr$navigate(paste0("https://us.vestiairecollective.com/women-bags/handbags/",
                          Designer_List$Designers[Index_designer],                     "/#sold=1_priceMin=",as.character(minPrice*100),"_priceMax=",as.character(maxPrice*100)))
      GoToSleep()
      print(paste0("starting to scrape between ",minPrice," and ",maxPrice))
      MoveThroughPages_ScrapeAndAppendToList(Link_list)

      minPrice <- minPrice + 50
      maxPrice <- maxPrice + 50
    }
    maxPrice <- maxPrice + 50
        while (stopMarker >= maxPrice & maxPrice <= 3000.01)
    {
      remDr$navigate(paste0("https://us.vestiairecollective.com/women-bags/handbags/",
                          Designer_List$Designers[Index_designer],                     "/#sold=1_priceMin=",as.character(minPrice*100),"_priceMax=",as.character(maxPrice*100)))
      GoToSleep()
      print(paste0("starting to scrape between ",minPrice," and ",maxPrice))
      MoveThroughPages_ScrapeAndAppendToList(Link_list)

      minPrice <- minPrice + 100
      maxPrice <- maxPrice + 100
    }
    maxPrice <- maxPrice + 900
    while (stopMarker >= maxPrice)
    {
      remDr$navigate(paste0("https://us.vestiairecollective.com/women-bags/handbags/",
                          Designer_List$Designers[Index_designer],                     "/#sold=1_priceMin=",as.character(minPrice*100),"_priceMax=",as.character(maxPrice*100)))
      GoToSleep()
      print(paste0("starting to scrape between ",minPrice," and ",maxPrice))
      MoveThroughPages_ScrapeAndAppendToList(Link_list)

      minPrice <- minPrice + 1000
      maxPrice <- maxPrice + 1000
    }
    remDr$navigate(paste0("https://us.vestiairecollective.com/women-bags/handbags/",
                          Designer_List$Designers[Index_designer],                     "/#sold=1_priceMin=",as.character(minPrice*100)))
    GoToSleep()
    paste("This is the last search for this desginer")
    MoveThroughPages_ScrapeAndAppendToList(Link_list)
    
  }
}

```
  
```{r Scraping content from individual links}  
    
  for (Index_product in 1:length(Link_list))
  {
    
    remDr$navigate(Link_list[i])
    
    print(paste0("scraping product number - ",Index_product))
    
    GoToSleep()
    
    Temp_Other_Details <- 
      remDr$findElements(using = "css", ".descriptionList__block__list li") 
    Temp_Likes <- 
      remDr$findElement(using = "css", ".productTitle__like")
    Temp_Price <- 
      remDr$findElement(using = "css", ".productPrice__sold")
  
    Other_Details[[Index_product]] <-
      sapply(Temp_Other_Details,function(x){x$getElementText()[[1]]})
    Likes[Index_product] <- 
      Temp_Likes$getElementText()
    Price[Index_product] <- 
      Temp_Price$getElementText()
    }


```

``` {r Functions for populating the dataframe}
# This function will help us speep up the process of extracting the correct strings from the "Other_Details" list.
# It locates an instance where there is a specific expression, then removing it from the original string.
LocateAndReplace <- function(String,Location)
{
      sub(String,"", 
       Location[which(grepl(String,Location) == TRUE)],
        Location)
}
```

``` {r Populating the dataframe}
df_index <- nrow(df) # As we will populate the dataframe several times, this will make sure we are not overwriting content

#Looping through each of the 3 variables we scraped
for (i in 1:length(Likes)) { # "Likes" could be replaced by "Price" or "Other_Details".
  
  df[df_index +i,"Date_sold"] <-
    regmatches(Price[i],
           regexpr("\\w+\\s{1}\\d{1,2},\\s{1}\\d{4}$",
                   Price[i],perl = TRUE))
  
  df[df_index +i,"Likes"] <-
    as.numeric(Likes[[i]])
  
  # When I can I will be using the "LocateAndReplace" function.
  df[df_index +i,"Designer"] <- 
    LocateAndReplace("Designer: ",Other_Details[[i]])
  
  try(df[df_index +i,"Model"] <- 
    LocateAndReplace("Model: ",Other_Details[[i]]), silent = T)
  
  df[df_index +i,"Price_sold"] <-
    regmatches(Price[i],regexpr("(?<=Sold at )\\d+.+(?= on)",
                                Price[i],perl = TRUE)) 
  # This is in euros
  
  df[df_index +i,"Condition"] <-
    LocateAndReplace("Condition: ",Other_Details[[i]])
  
  df[df_index +i,"Material"] <-
    LocateAndReplace("Material: ",Other_Details[[i]])
  
  df[df_index +i,"Color"] <-
    LocateAndReplace("Color: ",Other_Details[[i]])
  
  try(df[df_index +i,"Heel_height"] <-
    LocateAndReplace("Heel height: ",Other_Details[[i]]), silent = T) 
  # This will be empty for most observations
  
  df[df_index +i,"Size"] <-
    sub(" sizing guide", ""
        ,LocateAndReplace("Size: ",Other_Details[[i]]))
  # As the variable has some unwanted parts at the end of the string, 
  # we will preform another "sub" action before assigning to 
  # the dataframe
  
  df[df_index +i,"Location"] <-
    sub(",.+$","",
        LocateAndReplace("Location:",Other_Details[[i]]))
  # Again, we will preform another sub before assigning to the df.
  
  try(df[df_index +i,"Seller"] <-
    sub("^.+ from the seller ","",
        LocateAndReplace("Location:",Other_Details[[i]])), silent = T)
  
  df[df_index +i,"Sold_with"] <-
    paste(Other_Details[[i]][which(grepl(":",Other_Details[[i]]) == FALSE)],
          collapse = " ,")
  
    df[df_index +i,"Online_since"] <-
     LocateAndReplace("Online since: ",
                               Other_Details[[i]])
  
  df[df_index +i,"Reference"] <-
    LocateAndReplace("Reference: ",Other_Details[[i]])
  
  
  }
```


``` {r Testing Zone,include = FALSE}
main_webpage <- remDr$findElement("css", "body")
Next_button <- remDr$findElement(using = "css",
                               ".catalog__sortActionBar .catalogPagination__prevNextButton--next")
main_webpage$mouseMoveToLocation(webElement = Next_button) 
main_webpage$click(1)



#.catalogPagination__prevNextButton__text

      print(paste0("There are too many individual products in - ",
                 Designer_List$Designers[Index_designer],
                                     ", we move to the next one."))
    next()

for (x in c("hry","hhhh")) {
  if (x == "hhhh") {
   print(x)  
  }
 
}
      write.csv(Link_list,"LinkList.csv")
      SortByPriceAsc()
```

