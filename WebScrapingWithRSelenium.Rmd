---
title: "Scraping from vestiairecollective"
author: "Ran K"
date: "8/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(RSelenium)
library(rvest)
library(stringr)
# 1. Lunch docker and set up a chrome enironment, the command is "docker run --name chrome -v /dev/shm:/dev/shm -d -p 4445:4444 -p 5901:5900 selenium/standalone-chrome-debug:latest"
# 2. "docker ps -a" to verify that it is up
# 3. Lunch tightVNC and connect to out server.
```

```{r Server Preparation, echo = FALSE}
remDr <- remoteDriver(remoteServerAddr = "192.168.99.100", port = 4445L, browser = "chrome")
remDr$open()
```

```{r Website connection, echo=FALSE}
remDr$navigate("https://us.vestiairecollective.com/")
main_webpage <- remDr$findElement("css", "body")
Login_button <- remDr$findElement(using = "css", ".d-lg-none+ .d-lg-block .resetButton")
main_webpage$mouseMoveToLocation(webElement = Login_button) 
main_webpage$click(1)

# connecting to our account
Useremail_button <- remDr$findElement(using = "css", "#user_email")
Useremail_button$sendKeysToElement(list("WebScraperTest@sharklasers.com"))
Password_button <- remDr$findElement(using = "css", "#user_password")
Password_button$sendKeysToElement(list("TestTestTest"))
Connect_button <- remDr$findElement(using = "css", ".login__submit")
main_webpage <- remDr$findElement("css", "body")
main_webpage$mouseMoveToLocation(webElement = Connect_button) 
main_webpage$click(1)

# removing variables
rm(Login_button)
rm(Useremail_button)
rm(Password_button)
rm(Connect_button)
rm(main_webpage)
```

``` {r Functions we will use}
GoToSleep <- function() {
  print("Going to sleep")
  Sys.sleep(runif(1,4,6))
}

SortByPriceDesc <- function()
{
  sort_button <- remDr$findElement(using = "css",value = ".catalogSort__button")
  remDr$mouseMoveToLocation(webElement = sort_button)
  remDr$click(1)
  selection_button <- remDr$findElements(using = "css", value = ".mat-radio-label-content")
  selection_button <- selection_button[[3]]
  remDr$mouseMoveToLocation(webElement = selection_button)
  remDr$click(1)
  }

SortByPriceAsc <- function() 
  {
  sort_button <- remDr$findElement(using = "css",value = ".catalogSort__button")
  remDr$mouseMoveToLocation(webElement = sort_button)
  remDr$click(1)
  selection_button <- remDr$findElements(using = "css", value = ".mat-radio-label-content")
  selection_button <- selection_button[[2]]
  remDr$mouseMoveToLocation(webElement = selection_button)
  remDr$click(1)
  }

Next_page <- function() 
{
 # This will press the next page butten if available
  Next_button <- 
  remDr$findElement(using = "css", value = ".catalogPagination__prevNextButton--next .catalogPagination__prevNextButton__text")
  remDr$mouseMoveToLocation(webElement = Next_button)
  remDr$click(1)
}
  
SetMinPrice <- function(PriceMin) 
{
  Min_button <- remDr$findElement(using = "css",value = "#price-range-min")
  remDr$mouseMoveToLocation(webElement = Min_button)
  remDr$click(2)
  Min_button$sendKeysToElement(list(PriceMin))
}

SetMaxPrice <- function(PriceMax) 
{
  Max_button <- remDr$findElement(using = "css",value = "#price-range-max") 
  remDr$mouseMoveToLocation(webElement = Max_button)
  remDr$click(2)
  Max_button$sendKeysToElement(list(PriceMax))
} 

Detect_Next_Page_button <- function()
{
  Next_button <- try(remDr$findElement(using = "css", value = ".catalogPagination__prevNextButton--next .catalogPagination__prevNextButton__text"),silent = TRUE)
  if_else(class(Next_button) == "try-error", 0,1)
}

```

``` {r Initialing variables for scraping links}
# Initialising  the variables we will collect.
Link_list <- read.csv("LouisVuitton_Links.csv")
Link_list <- as.vector(Link_list[,1])
Price_list <- read.csv("LouisVuitton_Price.csv")
Price_list <- as.vector(Price_list[,1])
Designer_List <- read.csv ("DesignerList.csv")

Price_list <- vector()
Link_list <- vector()

Product_price <- list()
Likes <- list()
Other_Details <- list() # all other information (will be stored in a list)
Seller_Details <- list()


# This will be the database which will contain all the individual products
df <- 
  data.frame(Date_sold = as.character(),
             Likes = as.numeric(),
             Designer = as.character(),
             Model = as.character(),
             Price_sold = as.character(), 
             # As the site uses different currencies, ill do the exchange rate after.
             Condition = as.character(),
             Material = as.character(),
             Color = as.character(),
             Width = as.character(), # If noted
             Height = as.character(), # If noted
             Depth = as.character(), # If noted
             Location = as.character(),
             Seller = as.character(), # Is a part of the same string as location
             Sold_with = as.character(), # A vector of items
             Online_since = as.character(),
             Reference = as.numeric(),
             Seller_title = as.character(), # Taken from the seller detail variable
             Address = as.character()
             )
```

``` {r Preparing list of designers}
### Prepering the List of designers
#This is not needed as i have allready saved it to csv
remDr$navigate("https://us.vestiairecollective.com/women-bags/handbags")
Labels <- remDr$findElements(using = "css","label")
Labels <- sapply(Labels,function(x){x$getElementText()[[1]]})
Designer_List <- Labels[5:104] # This was checked manually
Designer_List <- as.data.frame(Designer_List)
Designer_List["Number"] <- as.numeric()
for (i in 1:nrow(Designer_List)) 
  {
  Designer_List$Number[i] <- as.numeric(regmatches(Designer_List[i,1],
                               regexpr("(?<=\\()\\d+(?=\\))",Designer_List[i,1],perl = TRUE)))
  Designer_List$Designers[i] <- gsub(" / | & "," ",Designer_List[i,1])
  Designer_List$Designers[i] <- gsub("[&\\.']","",Designer_List[i,3])
  Designer_List$Designers[i] <- gsub("[éè]","e",Designer_List[i,3])
  Designer_List$Designers[i] <- gsub("ï","i",Designer_List[i,3])
  Designer_List$Designers[i] <- gsub(" \\(\\d+\\)$","",Designer_List[i,3])
  Designer_List$Designers[i] <- gsub(" ","-",Designer_List[i,3])
  
   
}  

# Its possible to save it as Designer_List.csv
rm(Labels)
``` 

```{r improved scraping function}
MoveThroughPages_ScrapeAndAppendToList <- function(Link_list,Price_list,Price_marker)
  {
  print("Sorting by price")
  

  SortByPriceAsc()
  
  GoToSleep()
  
  Next_page_index <- 1
  while (Next_page_index == 1) {
    
    print(remDr$getCurrentUrl())
    GoToSleep() 
    
    print("Scraping links and prices")
    # Collecting the links for this individual page
    Links_in_page <- # Collecting all links
      remDr$findElements(using = "css", "meta") 
    Links_in_page <- # Grouping in a list
      unlist(sapply(Links_in_page,function(x){x$getElementAttribute('content')}))
    Links_in_page <- # Filtering to find only links to individual heals
      Links_in_page[which(grepl("https://us.vestiairecollective.com/women-bags/handbags/.+-\\d{3,}.shtml$",Links_in_page) == TRUE)]
   # Links_in_page <- gsub("^","https://",Links_in_page)
    
    # Collecting the price information in the page, we do this here to note what was the
    # initial price and what is the current price (for sold and unsold items)
    Prices_in_page <- 
      remDr$findElements(using = "css", value = ".productSnippet__price")
    Prices_in_page <- 
      unlist(sapply(Prices_in_page,function(x){x$getElementText()[[1]]}))
    
    print("Appending links and prices")
    # Adding the links and price information to our global variables
    Appended_item_index <- 0
    Updated_price_index <- 0
    for (i in 1:length(Links_in_page))
      {
      if (is.na(match(Links_in_page[i],Link_list)) == TRUE)
        {
        # Here we will check for duplicates
        # We will add append only if the link is unsued
      
        Link_list <- append(Link_list,Links_in_page[i])
        Price_list <- append(Price_list,Prices_in_page[i])
        Appended_item_index <- Appended_item_index + 1
        }
      else
        {
        Price_list[match(Links_in_page[i],Link_list)] <- Prices_in_page[i]
        # This will update price information when we run the script again.
        Updated_price_index <- Updated_price_index + 1
        }

      }
  print(paste0(Appended_item_index," items added and "
               ,Updated_price_index," Prices updated"))
  assign("Link_list",Link_list,envir = .GlobalEnv)
  assign("Price_list",Price_list,envir = .GlobalEnv)
  # This is to have the lists of links and prices updated in the global environment

  if (length(Links_in_page) == 0) 
    {
    print("There are no more links - breaking from the loop")
    break()
    }
  
  # We will keep track of the price of the last items that sold in the page, which will 
  # be the highest, as we are sorting by ascending price, and when we dont have 
  # anymore pages to scrape from, we will either move to another designer or use the
  # price marker to set a minimum price.
  

  Price_marker <- tail(Prices_in_page, n = 1) # Getting the last element
  Price_marker <- gsub(",",".",Price_marker) # Changing from coma to decimal point.
  Price_marker <- gsub(" ","",Price_marker) #Removing white space.
  Price_marker <- gsub("\\h","",Price_marker,perl = TRUE) #Removing thin white space.
  

  Price_marker <-
    regmatches(Price_marker,
           regexpr("(?<=\200)\\d+(?=\200$)|(?<=\200)\\d+\\.\\d+(?=\200$)|\\d+(?=\200$)|\\d+\\.\\d+(?=\200$)",
                   Price_marker,perl = TRUE))
  Price_marker <- as.numeric(gsub("\\h","",Price_marker,perl = TRUE)) #changing the thin whitespace and formatting as numeric
  
  # Locating only the instance of the second price if there are 2, and with or without 
  # a decimal point.
  print(paste0("our new minimum price is - ",Price_marker))
  assign("Price_marker",Price_marker,envir = .GlobalEnv)
  # Updating the price marker in the global enironment.
  
  Next_page_index <- Detect_Next_Page_button()
  
  if (Next_page_index == 1) 
  {
    print("next page button detected - moving on")
    Next_page()
  }
  
  
  } # closing while loop
  
} # closing function



```

``` {r Scraping links for future content scraping}
# add a for loop running on the designer list for a full list
for (Index_sold_item in 1:1) {
  remDr$navigate(paste0("https://us.vestiairecollective.com/women-bags/handbags/louis-vuitton/#sold=",Index_sold_item))
  GoToSleep()
  
  Maximum_price <- remDr$findElement(using = "css",value = 
                        ".vc-price-range__col--max .vc-price-range__label")
  Maximum_price <- unlist(Maximum_price$getElementText())
  Maximum_price <- gsub(" \200","",Maximum_price) # Removing euro sign
  Maximum_price <- gsub("[^0-9,]","",Maximum_price) # Removing euro sign
  Maximum_price <- as.numeric(gsub(",",".",Maximum_price))
  print(paste0("Stop marker is - ",Maximum_price))
  # This is the marker in which we do not need to scrape anymore.
  
  Price_marker <- 999.01
  # We will use the price marker to set the value on the minimal price.
  while (Maximum_price > Price_marker) {
    remDr$navigate(paste0("https://us.vestiairecollective.com/women-bags/handbags/louis-vuitton/#sold=",                Index_sold_item,"_priceMin=",as.character(Price_marker*100)))
    GoToSleep()
    print(paste0("starting to scrape from a minimum price of ",Price_marker))
    MoveThroughPages_ScrapeAndAppendToList(Link_list,Price_list,Price_marker)
  } # while loop

} # for loop sold item

rm(Maximum_price) # Removing uneeded variables
rm(Price_marker)
rm(Index_sold_item)
paste("Finished")
write.csv(Link_list,"LouisVuitton_Sold_Links.csv",row.names = FALSE,col.names = FALSE)
write.csv(Price_list,"LouisVuitton_Sold_Price.csv",row.names = FALSE,col.names = FALSE)
```

```{r Scraping content from individual links}  
temp_time <- Sys.time()
for (Index_product in 4000:5500)
{
  if (Index_product %% 100 == 0) {
    print(Sys.time()-temp_time)
    temp_time <- Sys.time()
  }
  if (Index_product %%10 == 0) {
    print("Deploying decoy")
    remDr$navigate("https://us.vestiairecollective.com/")
    Sys.sleep(2)
    
  }
  print(paste0("scraping product number - ",Index_product))
  remDr$navigate(as.character(Link_list[Index_product]))
  GoToSleep()
  if (remDr$getCurrentUrl() != as.character(Link_list[Index_product])) {
    print("This link is broken, moving on")
    next()
  }
  page <- read_html(remDr$getPageSource()[[1]])
  # Scraping text and appending

    # Treating the price variable
  #Product_price[[Index_product]] <- page %>% html_nodes(".productPrice__sold") %>% html_text()
  
  #if (length(Product_price[[Index_product]]) == 0) {Product_price[[Index_product]] <- page %>%      html_nodes(".productPrice__price") %>% html_text() }

  ###################
  #As I decided to only treat items that where sold,
  #there is no need to scrape the price as it was already taked care of
  #in the link scraping step.
  ###################
  
  
  # Treating likes variable
  Likes[Index_product] <- page %>% html_node(".productTitle__like") %>% html_text()
  
  # Treating the seller details
  Seller_Details[[Index_product]] <- 
    page %>% html_nodes(".sellerBlock__details div") %>% html_text()

  # Treating the block of extra details
  Other_Details[[Index_product]] <-
    page %>% html_nodes(".descriptionList__block__list li") %>% html_text()

  print("moving on")
  }


```

``` {r Functions for populating the dataframe}
# This function will help us speep up the process of extracting the correct strings from the "Other_Details" list.
# It locates an instance where there is a specific expression, then removing it from the original string.
LocateAndReplace <- function(String,Location)
{
      sub(String,"", 
       Location[which(grepl(String,Location) == TRUE)],
        Location)
}
```

``` {r Populating the dataframe}

#Looping through each of the 3 variables we scraped
 for (i in 2693:length(Link_list)) {
   if (is.null(Other_Details[[i]]) == TRUE) {
     next()
   }
   
   
### Treating the final price and the initial price from the price list string
   temp_price <- Price_list[i]
   temp_price <- gsub(",",".",
                     gsub(" ","",
                          gsub("\\h","",temp_price,perl = TRUE))) # Changing from coma to decimal point.

   

  df[i,"Price_sold"] <-
    as.numeric(regmatches(temp_price,
             regexpr("(?<=\200)\\d+(?=\200$)|(?<=\200)\\d+\\.\\d+(?=\200$)|\\d+(?=\200$)|\\d+\\.\\d+(?=\200$)",
                     temp_price,perl = TRUE))) 
   
   
  df[i,"Initial_price"] <-
    as.numeric(as.numeric(regmatches(temp_price,
             regexpr("^\\d+\\.\\d+(?=\200)|^\\d+(?=\200)",
                     temp_price,perl = TRUE)))) 
   
  df[i,"Likes"] <-
    as.numeric(Likes[[i]])
  
  # When I can I will be using the "LocateAndReplace" function.
  df[i,"Designer"] <- 
    LocateAndReplace("Designer: ",Other_Details[[i]])
  
  try(df[i,"Model"] <- 
    LocateAndReplace("Model: ",Other_Details[[i]]), silent = T)
  
  df[i,"Condition"] <-
    gsub("More infos","",LocateAndReplace("Condition: ",Other_Details[[i]]))
  
  df[i,"Material"] <-
    LocateAndReplace("Material: ",Other_Details[[i]])
  
  try(df[i,"Color"] <-
    LocateAndReplace("Color: ",Other_Details[[i]]), silent = T)
  
  try(df[i,"Width"] <-
    LocateAndReplace("Width: ",Other_Details[[i]]), silent = T) 
  # This will be empty for most observations
  
  try(df[i,"Height"] <- 
    LocateAndReplace("Height: ",Other_Details[[i]]),silent = T)
  
  try(df[i,"Depth"] <-
      LocateAndReplace("Depth: ",Other_Details[[i]]), silent = T)

  df[i,"Location"] <- # Using regular expressions to extract location
    gsub(",\\n.+\\n.+$","",
        LocateAndReplace("Location:",Other_Details[[i]]),perl = TRUE)
  
  df[i,"Seller"] <- # Using the Seller details variable
    Seller_Details[[i]][1]
    
  df[i,"Sold_with"] <-
    paste(Other_Details[[i]][which(grepl(":",Other_Details[[i]]) == FALSE)],
          collapse = " ,")
  
  
  # Sadly as i didnt scrape the price variable in each page i dont have this detail now.
  #df[df_index +i,"Date_sold"] <- regmatches(Price[i],regexpr("\\w+\\s{1}\\d{1,2},\\s{1}\\d{4}$",                   Price[i],perl = TRUE))
  
  df[i,"Online_since"] <-
     LocateAndReplace("Online since: ",
                               Other_Details[[i]])
  
  df[i,"Reference"] <-
    LocateAndReplace("Reference: ",Other_Details[[i]])
     
  df[i,"Link"] <-
    Link_list[i]
  
  df[i,"Seller_title"] <-
    sub(" \\d+.+$","",Seller_Details[[i]][match(table = grepl("sold",Seller_Details[[i]][]),x = TRUE)],perl = TRUE)
  }
```
```{r Saving}

write.csv(df,"Louis_vuitton_Final.csv",row.names = FALSE)


```

``` {r Testing Zone,include = FALSE}

    Maximum_price <- remDr$findElement(using = "css",value = 
                          ".vc-price-range__col--max .vc-price-range__label")
    Maximum_price <- unlist(Maximum_price$getElementText())
    Maximum_price <- as.numeric(gsub("[€$ ,]","",Maximum_price,perl = TRUE))


  

# Trying to find a button and if it is a try error we will break.
remDr$navigate(paste0("https://us.vestiairecollective.com/women-bags/handbags/"))
page <- remDr$getPageSource()[[1]]
page <- read_html(page)
page %>% html_nodes(".descriptionList__block__list li") %>% html_text()

  test <- 
    try(suppressMessages(page %>%
                           html_node(".productPrice__sold") %>%
                           html_text(), silent = T))
```

